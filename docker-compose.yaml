version: "3"

services:

  # ONLY CPU (Llama.cpp docker on MacOS doesn't support GPU/Metal)
  # server:
  #   image: ghcr.io/ggml-org/llama.cpp:server
  #   ports:
  #     - 8080:8080
  #   volumes:
  #     - ./models:/models
  #   environment:
  #     # alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
  #     LLAMA_ARG_MODEL: /models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf
  #     LLAMA_ARG_CTX_SIZE: 4096
  #     LLAMA_ARG_N_PARALLEL: 2
  #     LLAMA_ARG_ENDPOINT_METRICS: 1
  #     LLAMA_ARG_PORT: 8080

  llm_server:
    provider:
      type: model
      options:
        model: hf.co/bartowski/meta-llama-3.1-8b-instruct-gguf:q6_k
  
  app:
    container_name: app
    build: ./app
    ports:
      - "8000:8000"
    # restart: unless-stopped
    depends_on:
      - llm_server
      - redis

  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus/prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    restart: unless-stopped

  grafana:
    container_name: grafana
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin

  redis:
    container_name: redis
    hostname: redis
    image: redis/redis-stack-server:7.4.0-v2
    ports:
      - "6379:6379"
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 10s
      retries: 5
      start_period: 5s
      timeout: 5s